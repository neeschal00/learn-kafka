{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import StringType, StructType, StructField, FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, udf\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "import re\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Twitter sentiment analysis\").config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\").config(\"spark.mongodb.input.uri\",\"mongodb+srv://kells17:3wb4gxacK7w83QdN@cluster0.cqd2pe8.mongodb.net/?retryWrites=true&w=majority\").config(\"spark.mongodb.output.uri\",\"mongodb+srv://kells17:3wb4gxacK7w83QdN@cluster0.cqd2pe8.mongodb.net/?retryWrites=true&w=majority\").config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\").getOrCreate()\n",
    "\n",
    "df = spark.readStream .format(\"kafka\").option(\"kafka.bootstrap.servers\", \"172.16.5.4:9092\") .option(\"subscribe\", \"twitter\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySchema = StructType([StructField(\"text\", StringType(), True)])\n",
    "values = df.select(from_json(df.value.cast(\"string\"), mySchema).alias(\"tweet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(tweet: str) -> str:\n",
    "    tweet = re.sub(r'http\\S+', '', str(tweet))\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', str(tweet))\n",
    "    tweet = tweet.strip('[link]')\n",
    "\n",
    "    # remove users\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', str(tweet))\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', str(tweet))\n",
    "\n",
    "    # remove puntuation\n",
    "    my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@â'\n",
    "    tweet = re.sub('[' + my_punctuation + ']+', ' ', str(tweet))\n",
    "\n",
    "    # remove number\n",
    "    tweet = re.sub('([0-9]+)', '', str(tweet))\n",
    "\n",
    "    # remove hashtag\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', str(tweet))\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = values.select(\"tweet.*\")\n",
    "clean_tweets = F.udf(cleanTweet, StringType())\n",
    "raw_tweets = df1.withColumn('processed_text', clean_tweets(col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get the subjectifvity\n",
    "def getSubjectivity(tweet: str) -> float:\n",
    "    return TextBlob(tweet).sentiment.subjectivity\n",
    "\n",
    "\n",
    "# Create a function to get the polarity\n",
    "def getPolarity(tweet: str) -> float:\n",
    "    return TextBlob(tweet).sentiment.polarity\n",
    "\n",
    "\n",
    "def getSentiment(polarityValue: int) -> str:\n",
    "    if polarityValue < 0:\n",
    "        return 'Negative'\n",
    "    elif polarityValue == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectivity = F.udf(getSubjectivity, FloatType())\n",
    "polarity = F.udf(getPolarity, FloatType())\n",
    "sentiment = F.udf(getSentiment, StringType())\n",
    "\n",
    "subjectivity_tweets = raw_tweets.withColumn('subjectivity', subjectivity(col(\"processed_text\")))\n",
    "polarity_tweets = subjectivity_tweets.withColumn(\"polarity\", polarity(col(\"processed_text\")))\n",
    "sentiment_tweets = polarity_tweets.withColumn(\"sentiment\", sentiment(col(\"polarity\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_row_in_mongo(df):\n",
    "    mongoURL = \"mongodb+srv://kells17:3wb4gxacK7w83QdN@cluster0.cqd2pe8.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    df.write.format(\"mongo\").mode(\"append\").option(\"uri\", mongoURL).save()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/13 13:31:19 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-822a8932-f673-412c-bfc4-5667cc5048ac. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/13 13:31:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/11/13 13:33:03 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-d087a52b-50f1-4276-941e-8701ae63bee6-580003620-driver-0-1, groupId=spark-kafka-source-d087a52b-50f1-4276-941e-8701ae63bee6-580003620-driver-0] Connection to node -1 (/172.16.5.4:9092) could not be established. Broker may not be available.\n",
      "22/11/13 13:33:03 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-d087a52b-50f1-4276-941e-8701ae63bee6-580003620-driver-0-1, groupId=spark-kafka-source-d087a52b-50f1-4276-941e-8701ae63bee6-580003620-driver-0] Bootstrap broker 172.16.5.4:9092 (id: -1 rack: null) disconnected\n",
      "22/11/13 13:33:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-6f21b3e6-af40-409b-8637-b5bdfacce75e-1789584040-driver-0-2, groupId=spark-kafka-source-6f21b3e6-af40-409b-8637-b5bdfacce75e-1789584040-driver-0] Connection to node -1 (/172.16.5.4:9092) could not be established. Broker may not be available.\n",
      "22/11/13 13:33:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-6f21b3e6-af40-409b-8637-b5bdfacce75e-1789584040-driver-0-2, groupId=spark-kafka-source-6f21b3e6-af40-409b-8637-b5bdfacce75e-1789584040-driver-0] Bootstrap broker 172.16.5.4:9092 (id: -1 rack: null) disconnected\n",
      "22/11/13 13:35:15 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-d087a52b-50f1-4276-941e-8701ae63bee6-580003620-driver-0-1, groupId=spark-kafka-source-d087a52b-50f1-4276-941e-8701ae63bee6-580003620-driver-0] Connection to node -1 (/172.16.5.4:9092) could not be established. Broker may not be available.\n",
      "22/11/13 13:35:15 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-d087a52b-50f1-4276-941e-8701ae63bee6-580003620-driver-0-1, groupId=spark-kafka-source-d087a52b-50f1-4276-941e-8701ae63bee6-580003620-driver-0] Bootstrap broker 172.16.5.4:9092 (id: -1 rack: null) disconnected\n",
      "22/11/13 13:35:41 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-6f21b3e6-af40-409b-8637-b5bdfacce75e-1789584040-driver-0-2, groupId=spark-kafka-source-6f21b3e6-af40-409b-8637-b5bdfacce75e-1789584040-driver-0] Connection to node -1 (/172.16.5.4:9092) could not be established. Broker may not be available.\n",
      "22/11/13 13:35:41 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-6f21b3e6-af40-409b-8637-b5bdfacce75e-1789584040-driver-0-2, groupId=spark-kafka-source-6f21b3e6-af40-409b-8637-b5bdfacce75e-1789584040-driver-0] Bootstrap broker 172.16.5.4:9092 (id: -1 rack: null) disconnected\n",
      "22/11/13 13:37:26 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-d087a52b-50f1-4276-941e-8701ae63bee6-580003620-driver-0-1, groupId=spark-kafka-source-d087a52b-50f1-4276-941e-8701ae63bee6-580003620-driver-0] Connection to node -1 (/172.16.5.4:9092) could not be established. Broker may not be available.\n",
      "22/11/13 13:37:26 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-d087a52b-50f1-4276-941e-8701ae63bee6-580003620-driver-0-1, groupId=spark-kafka-source-d087a52b-50f1-4276-941e-8701ae63bee6-580003620-driver-0] Bootstrap broker 172.16.5.4:9092 (id: -1 rack: null) disconnected\n",
      "22/11/13 13:37:52 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-6f21b3e6-af40-409b-8637-b5bdfacce75e-1789584040-driver-0-2, groupId=spark-kafka-source-6f21b3e6-af40-409b-8637-b5bdfacce75e-1789584040-driver-0] Connection to node -1 (/172.16.5.4:9092) could not be established. Broker may not be available.\n",
      "22/11/13 13:37:52 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-6f21b3e6-af40-409b-8637-b5bdfacce75e-1789584040-driver-0-2, groupId=spark-kafka-source-6f21b3e6-af40-409b-8637-b5bdfacce75e-1789584040-driver-0] Bootstrap broker 172.16.5.4:9092 (id: -1 rack: null) disconnected\n",
      "22/11/13 13:39:37 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-d087a52b-50f1-4276-941e-8701ae63bee6-580003620-driver-0-1, groupId=spark-kafka-source-d087a52b-50f1-4276-941e-8701ae63bee6-580003620-driver-0] Connection to node -1 (/172.16.5.4:9092) could not be established. Broker may not be available.\n",
      "22/11/13 13:39:37 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-d087a52b-50f1-4276-941e-8701ae63bee6-580003620-driver-0-1, groupId=spark-kafka-source-d087a52b-50f1-4276-941e-8701ae63bee6-580003620-driver-0] Bootstrap broker 172.16.5.4:9092 (id: -1 rack: null) disconnected\n",
      "22/11/13 13:40:03 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-6f21b3e6-af40-409b-8637-b5bdfacce75e-1789584040-driver-0-2, groupId=spark-kafka-source-6f21b3e6-af40-409b-8637-b5bdfacce75e-1789584040-driver-0] Connection to node -1 (/172.16.5.4:9092) could not be established. Broker may not be available.\n",
      "22/11/13 13:40:03 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-6f21b3e6-af40-409b-8637-b5bdfacce75e-1789584040-driver-0-2, groupId=spark-kafka-source-6f21b3e6-af40-409b-8637-b5bdfacce75e-1789584040-driver-0] Bootstrap broker 172.16.5.4:9092 (id: -1 rack: null) disconnected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/.pyenv_mirror/user/current/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/workspace/.pyenv_mirror/user/current/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[39m=\u001b[39m sentiment_tweets\u001b[39m.\u001b[39mwriteStream\u001b[39m.\u001b[39mqueryName(\u001b[39m\"\u001b[39m\u001b[39mmessi\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mforeachBatch(write_row_in_mongo)\u001b[39m.\u001b[39mstart()\n\u001b[0;32m----> 2\u001b[0m query\u001b[39m.\u001b[39;49mawaitTermination()\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.8/site-packages/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsq\u001b[39m.\u001b[39mawaitTermination(\u001b[39mint\u001b[39m(timeout \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsq\u001b[39m.\u001b[39;49mawaitTermination()\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.8/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.8/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/workspace/.pyenv_mirror/user/current/lib/python3.8/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "query = sentiment_tweets.writeStream.queryName(\"messi\").foreachBatch(write_row_in_mongo).start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"TwitterSentimentAnalysis\")\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", \"twitter\") \\\n",
    "        .load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ac03a0a6051494cc606d484d27d20fce22fb7b4d169f583271e11d5ba46a56e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
