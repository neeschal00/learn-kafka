{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import StringType, StructType, StructField, FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, udf\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "import re\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up JAVA_TOOL_OPTIONS:  -Xmx3489m\n",
      "Picked up JAVA_TOOL_OPTIONS:  -Xmx3489m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/13 13:01:29 WARN Utils: Your hostname, neeschal00-learnkafka-l473aaliera resolves to a loopback address: 127.0.0.1; using 10.0.5.2 instead (on interface ceth0)\n",
      "22/11/13 13:01:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/workspace/.pyenv_mirror/user/current/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/gitpod/.ivy2/cache\n",
      "The jars for the packages stored in: /home/gitpod/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e1fabb3f-9c49-42e7-a89b-8fc6bc3e5be5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.1.2/spark-sql-kafka-0-10_2.12-3.1.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2!spark-sql-kafka-0-10_2.12.jar (61ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.1.2/spark-token-provider-kafka-0-10_2.12-3.1.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2!spark-token-provider-kafka-0-10_2.12.jar (21ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.6.0/kafka-clients-2.6.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.6.0!kafka-clients.jar (214ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (24ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (16ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.4.8-1/zstd-jni-1.4.8-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.4.8-1!zstd-jni.jar (156ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (28ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.2/snappy-java-1.1.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.2!snappy-java.jar(bundle) (56ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (17ms)\n",
      ":: resolution report :: resolve 2067ms :: artifacts dl 599ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   9   |   9   |   0   ||   9   |   9   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e1fabb3f-9c49-42e7-a89b-8fc6bc3e5be5\n",
      "\tconfs: [default]\n",
      "\t9 artifacts copied, 0 already retrieved (13083kB/17ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/13 13:01:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Twitter sentiment analysis\").config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\").config(\"spark.mongodb.input.uri\",\n",
    "    \"mongodb+srv://kells17:3wb4gxacK7w83QdN@cluster0.cqd2pe8.mongodb.net/?retryWrites=true&w=majority\"\n",
    ").config(\"spark.mongodb.output.uri\",\"mongodb+srv://kells17:3wb4gxacK7w83QdN@cluster0.cqd2pe8.mongodb.net/?retryWrites=true&w=majority\")..config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\").getOrCreate()\n",
    "\n",
    "df = spark.readStream .format(\"kafka\").option(\"kafka.bootstrap.servers\", \"172.16.5.4:9092\") .option(\"subscribe\", \"twitter\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySchema = StructType([StructField(\"text\", StringType(), True)])\n",
    "values = df.select(from_json(df.value.cast(\"string\"), mySchema).alias(\"tweet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(tweet: str) -> str:\n",
    "    tweet = re.sub(r'http\\S+', '', str(tweet))\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', str(tweet))\n",
    "    tweet = tweet.strip('[link]')\n",
    "\n",
    "    # remove users\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', str(tweet))\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', str(tweet))\n",
    "\n",
    "    # remove puntuation\n",
    "    my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@â'\n",
    "    tweet = re.sub('[' + my_punctuation + ']+', ' ', str(tweet))\n",
    "\n",
    "    # remove number\n",
    "    tweet = re.sub('([0-9]+)', '', str(tweet))\n",
    "\n",
    "    # remove hashtag\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', str(tweet))\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = values.select(\"tweet.*\")\n",
    "clean_tweets = F.udf(cleanTweet, StringType())\n",
    "raw_tweets = df1.withColumn('processed_text', clean_tweets(col(\"text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get the subjectifvity\n",
    "def getSubjectivity(tweet: str) -> float:\n",
    "    return TextBlob(tweet).sentiment.subjectivity\n",
    "\n",
    "\n",
    "# Create a function to get the polarity\n",
    "def getPolarity(tweet: str) -> float:\n",
    "    return TextBlob(tweet).sentiment.polarity\n",
    "\n",
    "\n",
    "def getSentiment(polarityValue: int) -> str:\n",
    "    if polarityValue < 0:\n",
    "        return 'Negative'\n",
    "    elif polarityValue == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectivity = F.udf(getSubjectivity, FloatType())\n",
    "polarity = F.udf(getPolarity, FloatType())\n",
    "sentiment = F.udf(getSentiment, StringType())\n",
    "\n",
    "subjectivity_tweets = raw_tweets.withColumn('subjectivity', subjectivity(col(\"processed_text\")))\n",
    "polarity_tweets = subjectivity_tweets.withColumn(\"polarity\", polarity(col(\"processed_text\")))\n",
    "sentiment_tweets = polarity_tweets.withColumn(\"sentiment\", sentiment(col(\"polarity\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"TwitterSentimentAnalysis\")\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", \"twitter\") \\\n",
    "        .load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ac03a0a6051494cc606d484d27d20fce22fb7b4d169f583271e11d5ba46a56e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
